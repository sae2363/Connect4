You will
submit your implementation (i.e., source code) along with a writeup explaining 1) what you
implemented, 2) your design decisions while doing so, 3) your experimental setup, and 4) your
observed quantitative experimental results.

Writeup of data seen and implementation of the game 

The game connect 4:

The file board contains the various methods needed to run a adversarial search program.  A array is stored to
represent the games board as a state with 0 being empty space, 1 being player one, and 2 being player two. When a game piece is placed it always starts at the top of the array and move down the colum if the space is 0 until it 
reaches a empty space. Actions are represented as point objects, the x value representing the colum and y the row it is being placed in. In the initial version of this game, actions would return every open spot so the y value was needed but it was subsequently removed as it was unneeded for performance reasons. The reason it is still in the code is point is needed to check a single spot to see if someone won to save on performance. More formally a state of the game is a np.array object.  The overall format of the code is very similar to assignment 1 code as I found it to be a good format.  The board.py implements the asp abstract class as well.  The board object can be constructed with how many rows, columns, and pieces in a row needed to win.  This I will get in to in more detail later for performance.

Main Classes
    MCTS.py: Monte Carlo Tree search code
    alphabeta.py:Alpha Beta pruning code
    nodeClass.py: Node to be used in the tree for MCTS
    point.py: data class that holds a x and a y value only
    board.py:  All the method to modify the board and also represent a adversarial search problem
    gui.py: the GUI version of the game
    cli.py: The command line version of the game

Main functions
    board
        -placePiece():makes a action on the board
        -checkWin(): checks the point to see if the player won
        -current_player(): given the state, give who has a move
        -isTerminal(): given the state, did someone win or is it possible to make a move
        -utility(): gives a value for a terminal state to be min/maxed 
    agents
        -choose_action(): Chooses a action with the state

There were no challenges for me to implement the classes outside over testing.  It was quick to get everything up and running but the fine-tuning took a while to do and I could have spent less time doing it.

The agents

There are 4 agents that are able to play the game
    - Alphabeta.py:  A AI agent based on alphabeta pruning search
    - MCTS.py: A AI agent based on Monte Carlos Tree search
    - randomAgent.py: A agent that plays a random action
    - realAgent.py: A agent that uses consol input to get a action

Note on python files

    Due to a slight issue I had with imports and running the files,  I made a copy of the classes for the files.  This was because some files in subdirectories could not access classes in the parent directories using VS code's run button. 

MCTS

My implementation of Monte Carlos Tree search is based on the AIMA books approach with UTC to improve further upon it. It works in the following steps to pick a action

    -Create a root node for the game tree with the current state
    -Sets a time limit for the algorithm (0.5-2 seconds)
    -creates the start of the tree for storing win data for at least one level
    -Repeat below steps until time runs out 
        -Finds the node with the highest UTC value
        -expands that node with a new leaf that contains a new state based on a random action
        -play out the game randomly from this point till the end and return who won
        -update the tree by going up it with who won
    -From the root node,  pick the child with the most playouts and use the action it contains as the chosen move

This approach work extremely well, as it is able to beat both me and my partner many times while running extremely fast.  More details in the data analysis section.  This algorithm took about 4 days and 2 hours each day to implement as I kept on tweaking the implementation provided in the book to see what works best.  My first version did work but had many errors which made it perform as good as a random agent.  After fixing the errors multiple times, it resulted in the version displayed here.  

Node class

The node used in the tree.  It stores many variables but the main ones are parent, nextNodes,state,total, totalUtil.  All of which are used in MCTS

Point class

The point class stores just a x and y value.  x is the colum and y is the row.  One confusing thing is it is initialized (row, colum) or in the case of how it is stored (y,x).  It also contains a method that is used to check if the place in the board that the point corresponds to is the number put in as a player

Experiment set up and how we got graph images 1 and 2

In the python file MCTS_graphs and bot_runner.py,  we test the various algorithm vs each other. The bot_runner.py is a simple script that goes back and fourth between the agents to run them and select moves.  Time is gather starting at when its the players turn and ends when it a action is received and placed.  This is then put in to a list to be used as the data points. graph 1-4 in each image is a simple win loss draw rate and 5+6 is how long each move took.

Data analysis on graph 1 and 2

Notes: 
    -With the size of the game being smaller,  the game is "solved" by the algorithm.  With this, P1 is able to almost always win.
    -Data graph 2 is the same as graph 1 but p1 and p2 are flipped and only 16 runs per graph verse 20 runs in graph 1


MCTS performance vs other agents
    -MCTS is able to win vs the other agents by a land slide 
    -It is even able to beat me quite often

Alphabeta performance vs other agents
    -It is able to win by a landslide verse a random agent showing it still is making smart decisions even when it lost to MCTS by a landslide
    -It is able to also beat Alfonso on a 4 by 4 board win by 3

MCTS time performance
    -MCTS is very fast especially at the first few moves for how well it performs
    -The time it has to decide is random between 0.5 and 2 seconds
    -the time it takes to make a move remains constant over time between the range above

Alphabeta
    -The time it take alphabeta to run is very exponential
    -This does decrease exponentially and it becomes faster then MCTS after a few moves

Notable thing about the graphs

    -Data_Graph_one
        -MC means MCTS,AB is Alpha beta, and random is a agent that picks random moves
        -Player one in this case always wins by a landslide 
        -graph 5 shows the exponential nature of the time it take alpha beta on a 4 by 4 board with win by 3
        -There is a outlier in the time it takes MCTS to run in the last graph as it should be constant
    -Data_Graph_two
        -Results here show player ones advantage over player 2 even when the agent used to play them is flipped 
        -One case where this is shown is in random vs alpha beta as they tie in how often they win verse the landslide if alphabeta went first
        -Time data was not taken this time as it should remain along a similar curve for both

How does this apply to the real world?
    The project models a simple 2 player game.  While it appears simple,  these games can have quite complex strategy's.  For a computer to be able to play and beat a real human is a achievement.  These agents can action as ways to play these game alone or provide a greater challenge.

How does the results tie in to properties of the algorithm?
    Given monte carlos method is playing out the game to the n rather then considering every move, it not as exponential as alphabeta.  This is how it is able to maintain its constant timing in the graph as it can finsh a run faster then it has time to move.  Alpha beta on the other hand has a actual exponential curve as it consider the square root of every path which for a game like connect 4 is a lot of options.  My way of thinking about it is also think about how the game tree looks for MCTS vs AB. For ab it has the majority of nodes from root to terminal states, each non terminal one branching out to more option.  For MCTS it has paths that extend from the root that most likely are not every possible path.  Also they don't branch out if its part of the play out call.





